{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=1000, api_key=api_key )\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# chroma db persist directory\n",
    "persist_directory = \"local_chroma_db\"\n",
    "\n",
    "# general information\n",
    "vectorstore_general = Chroma(persist_directory=persist_directory, embedding_function=embeddings, collection_name=\"general\")\n",
    "retriever_general = vectorstore_general.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "\n",
    " # sajith's manifesto\n",
    "vectorstore_sajith = Chroma(persist_directory=persist_directory, embedding_function=embeddings, collection_name=\"sajith_premadasa\")\n",
    "retriever_sajith = vectorstore_sajith.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# akd's manifesto\n",
    "vectorstore_akd = Chroma(persist_directory=persist_directory, embedding_function=embeddings, collection_name=\"anura_kumara_dissanayake\")\n",
    "retriever_akd = vectorstore_akd.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# ranil's manifesto\n",
    "vectorstore_ranil = Chroma(persist_directory=persist_directory, embedding_function=embeddings, collection_name=\"ranil_wickramasinghe\")\n",
    "retriever_ranil = vectorstore_ranil.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever_general = create_history_aware_retriever(\n",
    "    llm, retriever_general, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "history_aware_retriever_sajith = create_history_aware_retriever(\n",
    "    llm, retriever_sajith, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "history_aware_retriever_akd = create_history_aware_retriever(\n",
    "    llm, retriever_akd, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "history_aware_retriever_ranil = create_history_aware_retriever(\n",
    "    llm, retriever_ranil, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "retrievers = {\n",
    "    \"sajith_premadasa\": history_aware_retriever_sajith,\n",
    "    \"anura_kumara_dissanayake\": history_aware_retriever_akd,\n",
    "    \"ranil_wickramasinghe\": history_aware_retriever_ranil,\n",
    "}\n",
    "\n",
    "class SearchAndCompare(BaseModel):\n",
    "    \"\"\"Search for information about a person or compare informations about persons.\"\"\"\n",
    "    queryType: str = Field(\n",
    "        ...,\n",
    "        description=\"Query type. Should be `search` or `compare` or `general`. if there's only one person name it's search, if there are many person's name it's compare, or it can be a general question which does not require any specific person\",)\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Query to look up or query to compare\",\n",
    "    )\n",
    "\n",
    "    candidates: int = Field(\n",
    "        ...,\n",
    "        description=\"Number of persons to search or compare. can be 0 for general questions\",\n",
    "    )\n",
    "\n",
    "    person1: str = Field(\n",
    "        ...,\n",
    "        description=\"Person to look things up for or persons to compare. Should be `sajith_premadasa` or `anura_kumara_dissanayake` or `ranil_wickramasinghe` or can be 'null'.\",\n",
    "    )\n",
    "    person2: str = Field(\n",
    "        ...,\n",
    "        description=\"Person to look things up for or persons to compare. Should be `sajith_premadasa` or `anura_kumara_dissanayake` or `ranil_wickramasinghe` or can be 'null'.\",\n",
    "    )\n",
    "    person3: str = Field(\n",
    "        ...,\n",
    "        description=\"Person to look things up for or persons to compare. Should be `sajith_premadasa` or `anura_kumara_dissanayake` or `ranil_wickramasinghe` or can be 'null'.\",\n",
    "    )\n",
    "\n",
    "system_query = \"\"\"You have the ability to determine whether the user question is general, or it is related to a specific person or it is a comparison between multiple persons. if you can't find the type set it as general\"\"\"\n",
    "prompt_query = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_query),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_llm_query = llm.with_structured_output(SearchAndCompare)\n",
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt_query | structured_llm_query\n",
    "\n",
    "@chain\n",
    "def qa_chain(question):\n",
    "    response = query_analyzer.invoke(question)\n",
    "    # print(response)\n",
    "    if response.queryType == \"search\" or response.queryType == \"compare\":\n",
    "        if response.queryType == \"search\":\n",
    "            retriever = retrievers[response.person1]\n",
    "            retrieved_docs = retriever.invoke({\"input\":response.query, \"chat_history\": chat_history})\n",
    "\n",
    "            prompt = (\n",
    "            \"system :\"\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "            \"\\n\\n\"\n",
    "\n",
    "            \"chat_history :\" \n",
    "            \"{chat_history}\"\n",
    "\n",
    "            \"human :\"\n",
    "            \"{question}\"\n",
    "            ).format(context=retrieved_docs, question=question, chat_history=chat_history)\n",
    "\n",
    "            result = llm.invoke(prompt)\n",
    "\n",
    "            return result\n",
    "    \n",
    "        elif response.queryType == \"compare\":\n",
    "            retriever1 = retrievers[response.person1]\n",
    "            retrieved_docs1 = retriever1.invoke({\"input\":response.query, \"chat_history\": chat_history})\n",
    "\n",
    "            if response.person2 != 'null':\n",
    "                retriever2 = retrievers[response.person2]\n",
    "                retrieved_docs2 = retriever2.invoke({\"input\":response.query, \"chat_history\": chat_history})\n",
    "            else:\n",
    "                retrieved_docs2 = ''\n",
    "\n",
    "            if response.person3 != 'null':\n",
    "                retriever3 = retrievers[response.person3]\n",
    "                retrieved_docs3 = retriever3.invoke({\"input\":response.query, \"chat_history\": chat_history})\n",
    "            else:\n",
    "                retrieved_docs3 = ''\n",
    "\n",
    "            prompt = (\n",
    "            \"system :\"\n",
    "            \"You are an assistant for comparing manifestos. \"\n",
    "            \"Use the following pieces of retrieved context from different manifestos to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context1}\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context2}\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context3}\"\n",
    "            \"\\n\\n\"\n",
    "\n",
    "            \"chat_history :\" \n",
    "            \"{chat_history}\"\n",
    "\n",
    "            \"human :\"\n",
    "            \"{question}\"\n",
    "            ).format(context1=retrieved_docs1, context2=retrieved_docs2, context3=retrieved_docs3, question=question, chat_history=chat_history)\n",
    "\n",
    "            result = llm.invoke(prompt)\n",
    "\n",
    "            return result\n",
    "    else:\n",
    "        retriever = history_aware_retriever_general\n",
    "        retrieved_docs = retriever.invoke({\"input\":response.query, \"chat_history\": chat_history})\n",
    "\n",
    "        prompt = (\n",
    "            \"system :\"\n",
    "            \"You are an assistant for question-answering tasks related to srilankan election.\"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know.\"\n",
    "            \"or if the question is not much related to srilankan election say that this question is not related to srilankan election ass a election chatbot i can't provide you with answer this.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "            \"\\n\\n\"\n",
    "\n",
    "            \"chat_history :\" \n",
    "            \"{chat_history}\"\n",
    "\n",
    "            \"human :\"\n",
    "            \"{question}\"\n",
    "            ).format(context=retrieved_docs, question=question, chat_history=chat_history)\n",
    "\n",
    "        result = llm.invoke(prompt)\n",
    "        return result\n",
    "\n",
    "def chatbot(question):\n",
    "    result = qa_chain.invoke(question)\n",
    "    \n",
    "    # retains only last 3 conversations in history\n",
    "    if len(chat_history) == 6:\n",
    "        chat_history.pop(0)\n",
    "        chat_history.pop(0)\n",
    "    \n",
    "    chat_history.extend([\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=result.content),\n",
    "        ])\n",
    "    \n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This question is not related to Sri Lankan elections. As an election chatbot, I can't provide you with an answer to this. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "question = \"what is 9 plus 4\"\n",
    "display(Markdown(chatbot(question)))\n",
    "print(len(chat_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
